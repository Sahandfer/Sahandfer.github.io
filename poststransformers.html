<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ‘¾&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Transformers: What You Really Need&nbsp;|&nbsp;sa[ML]og</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Transformers: What You Really Need">
  
    <meta name="description" content="In this post, I will go through all you need to know to get started on transformers. The architecture not the movie.">
    <meta property="og:description" content="In this post, I will go through all you need to know to get started on transformers. The architecture not the movie.">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ¤–&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ‘¾&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="research.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ”¬&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>Research</span>
        </div>
      </a>
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ˜€&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ¤–&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">Transformers: What You Really Need</h1>
    
  </header>
  <article id="https://www.notion.so/08966f5163c84df3886fc900868c2abb" class="PageRoot"><h1 id="https://www.notion.so/bbd53ce9f73c42ffbde99eb0bf3bc8e4" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/bbd53ce9f73c42ffbde99eb0bf3bc8e4"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">May I have your attention, please?</strong></span></span></h1><div id="https://www.notion.so/5b43130bf10342d3ab910515bce7de44" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">If you&#x27;ve spent more than 2 hours on an NLP course, it&#x27;s likely that you&#x27;ve heard someone mentionÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">transformers</em></span><span class="SemanticString">Â or a paper calledÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Attention is All You Need</em></span><span class="SemanticString">Â [1]. Transformers have become the building blocks of many machine learning applications, especially in the field of Natural Language Processing, to the point that it&#x27;s hard to find papers that neither use nor mention them in some way, shape or form. In this article, I&#x27;ll be giving, or at least try to give, a comprehensive review of what transformers are, how they came to be, and how they have evolved since they first appeared in [1]. So sit back, relax, and let&#x27;s dive in.</span></span></p></div><h2 id="https://www.notion.so/4975e124ff5e4ef2bfb1b495413d0a7a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/4975e124ff5e4ef2bfb1b495413d0a7a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Seq2Seq</strong></span></span></h2><div id="https://www.notion.so/b256659fe603453188db9aa6d733dbab" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We don&#x27;t have time for a name, just call it sequence to sequence, but cooler.</span></span></p></div><h3 id="https://www.notion.so/3e908b89dcf944e8b6407528bc87cad2" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/3e908b89dcf944e8b6407528bc87cad2"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">A bit of history</strong></span></span></h3><div id="https://www.notion.so/c6d91fe8d78c472e84e2425e4e18105b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">It&#x27;s the fall of 2014. Google&#x27;s killing the translation game with Google Translate and 15-year-old me still has no clue how they do it. Back around those times, they releaseÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Sequence to Sequence Learning with Neural Networks</em></span><span class="SemanticString">Â [2], which basically said Deep Learning and Neural Networks are great and all, but we want an architecture that maps one sequence to the other. Why is that needed, you say? Well, look at the following two sentences:</span></span></p></div><blockquote id="https://www.notion.so/cda283246bbb41c8b7efa3947424b646" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">English: Jay Alammar makes better articles than you.</span></span><div id="https://www.notion.so/ebb2efeb5c5b4dfcb7b311103321ac3d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Chinese: Jay Alammar ä½œçš„æ–‡ç« æ¯”ä½ çš„å¥½ã€‚</span></span></p></div></blockquote><div id="https://www.notion.so/5af3b05b14e74dedae9e48f10920c6b4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">If I was making a machine that could translate text from English to Chinese, or vice versa, wouldn&#x27;t it be convenient to have a machine that could learn the mapping between these two sequences? And that&#x27;s exactly what they did. They used one Long Short Term Memory (LSTM) unit, to map the first sequence to a vector with a fixed dimension, known as the context vector. This process is referred to asÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">encoding</em></span><span class="SemanticString">. Then, they use a second LSTM to find the target sequence from this context vector, which is referred to asÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">decoding</em></span><span class="SemanticString">. At this point, one might imagine if Vaswani, et al. had come up with this idea, the paper would have probably have been titledÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Encoding/Decoding is All You Need</em></span><span class="SemanticString">.</span></span></p></div><h3 id="https://www.notion.so/ac59cae9900748b28006b50d7e27db3f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/ac59cae9900748b28006b50d7e27db3f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Applications</strong></span></span></h3><div id="https://www.notion.so/cb6395e4855b4a478d445337f085fe29" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">I know I&#x27;ve only touched upon machine translation, but there are other applications that can benefit from mapping one sequence to the other. Let&#x27;s say that in the above example, the English sentence is given and I ask you the question: &quot;Well, how do you say that in Chinese?&quot;. Now, by translating this sentence, you have actually answered my question as well. You see where I&#x27;m going with this? Now, let&#x27;s say we use &quot;What is the capital of China?&quot; as the first sentence and &quot;Beijing&quot; as the second. Then learning a mapping between these two and similar pairs enables us to answer questions! Well how about we use one turn of the conversation as the first sentence and the reply to that turn as the second. Then the model learns how to respond to your statements (hypothetically, of course, there are other things to consider here). So, we end up with at least the following NLP applications for Seq2Seq:</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/7fc01009812d4e579ef4e71a29ef952e" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Machine Translation</span></span></li><li id="https://www.notion.so/52fc2a26a0fa4cf6b30908a0747d3f36" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">Question Answering</span></span></li><li id="https://www.notion.so/3861281bb64e463bb3dd805afa8acd14" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">Conversational Modeling</span></span></li></ol><div id="https://www.notion.so/37e2dbb12e7e4f2e9cd6370a3256a721" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">This architecture has become a fundamental part of many NLP applications and is still being used in 2021. That&#x27;s why I&#x27;m going to talk about the math behind it now, so this is your chance to skip to the next part if you&#x27;re not a big fan of equations. I&#x27;ll give you 3 seconds to decide.</span></span></p></div><h3 id="https://www.notion.so/108f9ba20d9a4e238b14ac46b0eb61c1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/108f9ba20d9a4e238b14ac46b0eb61c1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">How does it work?</strong></span></span></h3><div id="https://www.notion.so/e2bbce8bfb2446f9b210c95be1d3fcde" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Glad you decided to come here. Let&#x27;s say we have the following two sequences</span></span></p></div><p id="https://www.notion.so/4a96764aa328488e81c17076efb0cdc2" class="Equation" data-latex="S = (s_1, s_2, s_3, ..., s_n) \quad \text{and} \quad T = (t_1, t_2, t_3, ..., t_n)"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>3</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mspace width="1em"/><mtext>and</mtext><mspace width="1em"/><mi>T</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>3</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S = (s_1, s_2, s_3, ..., s_n) \quad \text{and} \quad T = (t_1, t_2, t_3, ..., t_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord">and</span></span><span class="mspace" style="margin-right:1em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><div id="https://www.notion.so/eea29100198a468eb63b35babf5300b8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Where S is the input sequence and T is the target sequence. In the case of the mentioned translation, the English sentence would be S and the Chinese translation would be T. Therefore, our model should estimate the probability of outputing T, given that the input sequence is S.</span></span></p></div><p id="https://www.notion.so/86d6611905e649f697803187be911135" class="Equation" data-latex="p(T|S) = p(t_1, t_2,t_3, ..., t_n|s_1, s_2, s_3, ..., s_n)Â "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>T</mi><mi mathvariant="normal">âˆ£</mi><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>3</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>n</mi></msub><mi mathvariant="normal">âˆ£</mi><msub><mi>s</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>3</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mtext>Â </mtext></mrow><annotation encoding="application/x-tex">p(T|S) = p(t_1, t_2,t_3, ..., t_n|s_1, s_2, s_3, ..., s_n)Â </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord">âˆ£</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">Â </span></span></span></span></span></p><div id="https://www.notion.so/9009a84f7e354c8e8b73b839dbb32121" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Initially, the LSTM unit encodes sequence S to get a context vector representation C, which is the last hidden state of the unit. Now that we have V, we start decoding and generate the output sequence O one word at a time, knowing that this is the RNN fashion for producing the output.</span></span></p></div><div id="https://www.notion.so/e29d690d4d464c389c196c7e6cee106f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">$$ \text{Assuming}\quad O = o_1, o_2, o_3, ..., o_n $$</span></span></p></div><div id="https://www.notion.so/62c68edf520749578569c9fb7aadae4a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">$$ \text{We find}\quad p(O|S)=p(O|C) = \prod_{m=1}^n p(o_m|C, o_1, o_2, ..., o_{m-1}) $$</span></span></p></div><div id="https://www.notion.so/8a82ed4703c24299a0d559d1f441c987" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Evidently, the model&#x27;s not going to get everything right from the beginning. That&#x27;s why we didn&#x27;t directly get T as the output sequence. Our job now is to look at the output sequence O, compare it to the sequence we want it to give us (i.e. target sequence T), and penalize it for not getting the right sequence until it learns the mapping. This could be done using Stochastic Gradient Descent.</span></span></p></div><h3 id="https://www.notion.so/be7317b388b548cb8b732ea38cf4449a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/be7317b388b548cb8b732ea38cf4449a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Something&#x27;s wrong, I can feel it</strong></span></span></h3><div id="https://www.notion.so/70744b146f93425a9311d9855dce7086" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">I mentioned how the model first converts the input sequence to a vector representation, which happens to have a fixed dimension. Well, it turned out that this made it challenging to deal with long sequences. Why, you may ask? Put yourself in the encoder&#x27;s shoes and imagine that I am a talkative friend who just told you all about his day and asked you to summarize it all in one sentence (this would be the same as fitting a long sequence into a fixed dimension vector). You would be devastated, right? and not necessarily because you didn&#x27;t really care when you asked me how my day was and just wanted to be polite, but rather because it&#x27;s so hard, maybe even impossible, to include every significant detail in just one sentence and not miss anything important. Now, think you&#x27;re a decoder and your encoder friend somehow managed to give you a sentence summarizing my day and now, you have to reconstruct the whole day and everything that happened using that one sentence. As sweat comes down your forehead and you sigh with exhaustion, you ask yourselfÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">isn&#x27;t there be a better way to deal with these things?</em></span></span></p></div><h2 id="https://www.notion.so/7796a9667e9041eeb4e469a36441a888" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/7796a9667e9041eeb4e469a36441a888"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Attention</strong></span></span></h2><div id="https://www.notion.so/f41a98bbfce74c818da3409f74c65054" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Can I have your attention please?</span></span></p></div><h3 id="https://www.notion.so/d8a8aa228d73469aa78f159d27fe00c1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/d8a8aa228d73469aa78f159d27fe00c1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">It&#x27;s just a bunch of weights</strong></span></span></h3><div id="https://www.notion.so/2070c0212030475a88ef7b1c98ec0256" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">In 2015, Bahdanau, et al. [3] came up with theÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">attention</em></span><span class="SemanticString">Â mechanism to address the problem of dealing with long sequences in machine translation. Essentially, this approach suggests that we don&#x27;t really need to encode the full sentence to a context vector. Instead, they say how about we let the decoder choose which part to payÂ </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">attention</strong></span><span class="SemanticString">Â to at each time step. If want to know how this is actually achieved, let&#x27;s delve deeper into the math behind it.</span></span></p></div><h3 id="https://www.notion.so/20409f1cb6174c9d91031c3020fae931" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/20409f1cb6174c9d91031c3020fae931"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">More equations</strong></span></span></h3><div id="https://www.notion.so/819dece1d59843fd844aa3b3d289d8eb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Let&#x27;s use the same terminology as before. We have a sequence S as input (with n words) and we want to get a sequence T as output. In this paper, they use a bidirectional RNN encoder, which gives two hidden states for each time step (forward and backward). The encoder state h</span><span class="SemanticString">i</span><span class="SemanticString">Â is assumed as the concatenation of these two states. Accordingly, the context vector is calculated as the following weighted average:</span></span></p></div><div id="https://www.notion.so/b85e84d0b3124a32b84f6264bf6c9aa5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">$$ c_i= \sum_{j=1}^{n} a_{ij}h_i $$</span></span></p></div><div id="https://www.notion.so/b0c919921c11472da8c842f0fd2d8118" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">$$ \text{where} \quad a_{ij} = softmax(e_{ij}) = \frac{exp(e_{ij})}{\sum_{k=1}^{n}exp(e_{ik})} $$</span></span></p></div><div id="https://www.notion.so/0c8199d5c3d844feaf4b04104cad0f2a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">$$ \text{and}\quad e_{ij} = am(s_{i-1}, h_j) $$</span></span></p></div><div id="https://www.notion.so/1773963d157d43c58b3d589d55064148" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">a</span><span class="SemanticString">ij</span><span class="SemanticString">Â is the weight showing how much of the input at input position j should be considered for output position i. In addition, am is an alignment model that tells us how well the input at position jand output at position i match. This is a feed-forward neural network with tanh activation, which is trained alongside other parts of the model. Hence, this term can be expanded as follows:</span></span></p></div><div id="https://www.notion.so/5ccab9026a784d6e9af7084eafc586f8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">$$ e_{ij} = am(s_{i-1}, h_j) = v_a^Ttanh(W_a(s_{i-1})+U_a(h_j)) $$</span></span></p></div><div id="https://www.notion.so/efb57bb308f441159a7a7ec23c23ce16" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">$$ \text{where }v_a^T, W_a, \text{ and }U_a \text{ are trainable parameters } $$</span></span></p></div><h3 id="https://www.notion.so/42f5b1b5b6f748f6a33444a89ec1076d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/42f5b1b5b6f748f6a33444a89ec1076d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">What did it cost?</strong></span></span></h3><div id="https://www.notion.so/179612d8c77b46ddb6a85bf1d220b3b2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">If you were one of the few people that read the above equations, you would notice that we need to calculate a</span><span class="SemanticString">ij</span><span class="SemanticString">Â for every single input and output position pairs i and j. Now, let&#x27;s say we have 100 words in the input and we plan to generate a sequence of 100 words. Simple combinatorics tells us that in this case we need to calculate 100*100 = 10000 values. If you&#x27;re someone that gets bugged with this time of computational complexity, then maybe you need something more than just simple attention.</span></span></p></div><h2 id="https://www.notion.so/78aeea3c2698410a8a34a3b8f6d50f55" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/78aeea3c2698410a8a34a3b8f6d50f55"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Self-Attention</strong></span></span></h2><div id="https://www.notion.so/76dfda0a1c3f4a4386a2fb9184ca91ca" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">It&#x27;s the year 2016. While many people are concerned about the american election results, the paperÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">Long Short-Term Memory-Networks for Machine Reading</em></span><span class="SemanticString">Â [4] comes out and introduces a concept we now know and love asÂ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">self-attention</em></span><span class="SemanticString">. As the title suggests, they use this mechanism to do machine reading and the reason they used it was because they wanted to find out the relation of each word in a sequence to the previous words in the same sequence. For instance, if a machine is reading the sentence &quot;I have a cat and it hates me&quot;, we want it to know what it refers to (the word &quot;cat&quot; in this case). So, how can we realize self-attention?</span></span></p></div><h3 id="https://www.notion.so/318d71f46a614f93b0c9e59268c045b3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/318d71f46a614f93b0c9e59268c045b3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Mafs</strong></span></span></h3><div id="https://www.notion.so/190c775e4b1c44bcab849f4090c18c14" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The self-attention mechanism relies on three vectors of a fixed dimension D: key (K), value (V) and query (Q). Matrices containing these vectors (namely key, value, and query matrices) are usually randomly initialized and then trained during the training process. Then, the we follow the below process to calculate self-attention.</span></span></p></div><div id="https://www.notion.so/854f5c6b5c144beead289ca1d75a7d03" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">1.</span></span></p></div><div id="https://www.notion.so/3259eab3cc69423cb015a2fb136a0dec" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We go back to 2013 and ask mikolov [5] if we could borrow his word embeddings. Then, we look up the word vector of each word in the input sequence using this embedding. Then, we multiply this word vector by the three matrices respectively to get the K, V, Q vectors.</span></span></p></div><div id="https://www.notion.so/0abff05bc1ed4e5384fdbea41b031ae2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">2.</span></span></p></div><div id="https://www.notion.so/c1094cf88bc44597bae4fd25589dc35d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We calculate a score S as the dot product of K and Q vectors.</span></span></p></div><div id="https://www.notion.so/ee39d2323a6d46fd862f9259de328194" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">3.</span></span></p></div><div id="https://www.notion.so/97b091b8ca504453acf633ba64626cc9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Divide the scores by square root of D. Apparently this helps with the gradient calculation. When you&#x27;re done doing that, do a softmax on all the scores because we want a nice positive distribution that sums up to 1.</span></span></p></div><div id="https://www.notion.so/d8bee94969cb4d72a98177106153a5b8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">4.</span></span></p></div><div id="https://www.notion.so/5bc73012fb5c4844bec75d710468d42e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Now, multiply V by the scores you just got. Why? because now you know what values to focus on: your score certainly got my attention.</span></span></p></div><div id="https://www.notion.so/f59ab08f2de6496db2a6772e16db8d2a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">5.</span></span></p></div><div id="https://www.notion.so/8681af29b22b4aafa0c613987168821d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Look at the sky and take a deep breath.Â </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">You got this!</strong></span></span></p></div><div id="https://www.notion.so/13fed5dc8f7d47b7adfdff51367273e7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">6.</span></span></p></div><div id="https://www.notion.so/380d70283e0d4022bfde1e64226ad0ca" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Calculate the sum of V.</span></span></p></div><h2 id="https://www.notion.so/a29bb81baeea45cb8ce0653dee224bc3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/a29bb81baeea45cb8ce0653dee224bc3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Transformers</strong></span></span></h2><div id="https://www.notion.so/db786a363b9b45ff8da1aaf37d4a756d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">If you found this article useful, you can cite it in your work as follows:</span></span></p></div><div id="https://www.notion.so/3470dd33eb7846d795f87c1650234aab" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"></span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">@article{sam2021transformers,
  title   = &quot;Is Attention Really All We Need? A comprehensive review of Transformers&quot;,
  author  = &quot;Sabour, Sahand&quot;,
  journal = &quot;saMLog&quot;,
  year    = &quot;2021&quot;,
  month   = &quot;7&quot;,
  url     = &quot;https://sahandfer.github.io/posts/Transformers/&quot;
}</code></span></span></p></div><h1 id="https://www.notion.so/3d9b6651e54f40f2b53ff0f7a2163d69" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/3d9b6651e54f40f2b53ff0f7a2163d69"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">References</strong></span></span></h1><div id="https://www.notion.so/a60d8e29f9ae49abac6e9e7e3ebdbd70" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">[1]</span></span></p></div></article>
  <footer class="Footer">
  <div>&copy; sa[ML]og 2019-2021</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>